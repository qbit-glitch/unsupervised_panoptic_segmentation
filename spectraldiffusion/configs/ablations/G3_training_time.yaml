# G3: Training Time
# Convergence speed comparison

# Variant 1: Full model (baseline)
G3_full:
  model:
    backbone: "base"
    num_slots: 12
    init_mode: "spectral"
    use_mamba: true
    use_diffusion: true
  data:
    dataset: "synthetic"
    image_size: [128, 128]
    batch_size: 16
  training:
    epochs: 50
    lr: 1e-4
  benchmark:
    measure_convergence: true
    target_ari: 0.8  # When to stop
  logging:
    exp_name: "ablation_G3_full"

---
# Variant 2: Without spectral init (slower convergence expected)
G3_no_spectral:
  model:
    backbone: "base"
    num_slots: 12
    init_mode: "random"
    use_mamba: true
    use_diffusion: true
  data:
    dataset: "synthetic"
    image_size: [128, 128]
    batch_size: 16
  training:
    epochs: 50
    lr: 1e-4
  benchmark:
    measure_convergence: true
    target_ari: 0.8
  logging:
    exp_name: "ablation_G3_no_spectral"

---
# Variant 3: Without Mamba (slower for large N)
G3_no_mamba:
  model:
    backbone: "base"
    num_slots: 12
    init_mode: "spectral"
    use_mamba: false
    use_diffusion: true
  data:
    dataset: "synthetic"
    image_size: [128, 128]
    batch_size: 16
  training:
    epochs: 50
    lr: 1e-4
  benchmark:
    measure_convergence: true
    target_ari: 0.8
  logging:
    exp_name: "ablation_G3_no_mamba"

---
# Variant 4: Simplified decoder (faster per step)
G3_mlp_decoder:
  model:
    backbone: "base"
    num_slots: 12
    init_mode: "spectral"
    use_mamba: true
    use_diffusion: false
  data:
    dataset: "synthetic"
    image_size: [128, 128]
    batch_size: 16
  training:
    epochs: 50
    lr: 1e-4
  benchmark:
    measure_convergence: true
    target_ari: 0.8
  logging:
    exp_name: "ablation_G3_mlp_decoder"
